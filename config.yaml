




# ====================================================================================
# General Parameters
# ====================================================================================

wandb_enabled: False
wandb_project_name: "Model-Misspecification"
wandb_exp_name: "Experiment 2"

early_stopping_enabled: False

device: "cuda"
noise_std: 0.
dh: 10
test_trials: 10 # How many test datasets to average results over / classical models to train

lambda_mle: 0.12 # value for the regularizing term of mle. has to be scaled by dataset size

# ====================================================================================
# Parameters for Classical Models
# ====================================================================================

num_iters_classical: 1
lr_classical: 0.001
dataset_size_classical: 50
weight_decay_classical: 0.0 # use optimizer-based weight decay instead

# ====================================================================================
# Parameters for In-Context Models
# ====================================================================================

num_iters_in_context: 100000
lr_in_context: 0.0001
dataset_size_in_context: 128
dataset_amount: 100000
weight_decay_in_context: 0.0001
batch_size_in_context: 100