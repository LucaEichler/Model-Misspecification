




# ====================================================================================
# General Parameters
# ====================================================================================

wandb_enabled: False
wandb_project_name: "Model-Misspecification"
wandb_exp_name: "Experiment 2 - 1M iters"


device: "cuda"
noise_std: 0.
dh: 100
test_trials: 10 # How many test datasets to average results over / classical models to train

lambda_mle: 0.12 # value for the regularizing term of mle. has to be scaled by dataset size

load_best_model: True # True: load the model with the best validation loss during training, False: keep the model right after ending training

# ====================================================================================
# Parameters for Classical Models
# ====================================================================================

num_iters_classical: 100000
lr_classical: 0.01
dataset_size_classical: 50
weight_decay_classical: 0.001 # use optimizer-based weight decay instead

# ====================================================================================
# Parameters for In-Context Models
# ====================================================================================

num_iters_in_context: 1000000 #1000000 #100000
lr_in_context: 0.0001
dataset_size_in_context: 128
dataset_amount: 100000
weight_decay_in_context: 0.0001
batch_size_in_context: 100

# ====================================================================================
# Parameters for Early Stopping
# ====================================================================================

early_stopping_enabled: False
early_stopping_patience: 10.
early_stopping_delta: 0.01